
ü¶ôüå≤ü§è Guanaco-LoRA: Low-Rank LLaMA Instruct-Tuning
---------------------------------------------------

This repository is a fork of the [Alpaca-LoRA repository](https://github.com/tloen/alpaca-lora), modified to include a custom dataset for coding generated by parsing data from StackOverflow. The dataset is mainly focused on JavaScript, which accounts for 90% of the data.

The custom dataset consists of 39,505 objects with a total of 42.36 million tokens. On average, each object contains 1,072.21 tokens.

### Dataset Statistics

*   Objects: 39,505
*   Total Tokens: 42.36M (TODO: check this)
*   Average Tokens per Object: 1,072.21

### Credits

The original repository was created by [tloen](https://github.com/tloen), and the results were reproduced using low-rank adaptation (LoRA), as described in the paper [Low-Rank Adaptation of Large Language Models: Non-Asymptotic Analysis and Practical Algorithms](https://arxiv.org/pdf/2106.09685.pdf).

#### Original Repository

The link to the original repository can be found [here](https://github.com/tloen/alpaca-lora).

### Setup

1. Install dependencies

```
pip install -r requirements.txt
```

2. If bitsandbytes doesn't work, [install it from source.](https://github.com/TimDettmers/bitsandbytes/blob/main/compile_from_source.md) Windows users can follow [these instructions](https://github.com/tloen/alpaca-lora/issues/17).

### Inference (`generate.py`)

This file reads the foundation model from the Hugging Face model hub and the LoRA weights from `tloen/alpaca-lora-7b`, and runs a Gradio interface for inference on a specified input. Users should treat this as example code for the use of the model, and modify it as needed.

### Training (`finetune.py`)

This file contains a straightforward application of PEFT to the LLaMA model,
as well as some code related to prompt construction and tokenization.
Near the top of this file is a set of hardcoded hyperparameters that you should feel free to modify.
PRs adapting this code to support larger models are always welcome.

### Checkpoint export (`export_*_checkpoint.py`)

These files contain scripts that merge the LoRA weights back into the base model
for export to Hugging Face format and to PyTorch `state_dicts`.
They should help users
who want to run inference in projects like [llama.cpp](https://github.com/ggerganov/llama.cpp)
or [alpaca.cpp](https://github.com/antimatter15/alpaca.cpp).
